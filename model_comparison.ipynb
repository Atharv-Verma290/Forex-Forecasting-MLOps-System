{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedf1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cca692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5080\")\n",
    "client = MlflowClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ceff824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_by_version(model_name: str, version: str):\n",
    "    model_uri = f\"models:/{model_name}/{version}\"\n",
    "    return mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962a0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenger_model(model_name):\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "    challengers = [\n",
    "        v for v in versions\n",
    "        if v.current_stage == \"None\"\n",
    "        and v.tags.get(\"candidate\") == \"challenger\"\n",
    "    ]\n",
    "    if not challengers:\n",
    "        raise RuntimeError(\"No challenger model found\")\n",
    "    \n",
    "    challenger = max(challengers, key=lambda v: int(v.version))\n",
    "\n",
    "    model = load_model_by_version(model_name, challenger.version)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"model_name\": model_name,\n",
    "        \"model_version\": challenger.version,\n",
    "        \"run_id\": challenger.run_id\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49721ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_production_model(model_name):\n",
    "    prod_versions = client.get_latest_versions(name=model_name, stages=[\"Production\"])\n",
    "\n",
    "    if not prod_versions:\n",
    "        print(f\"[INFO] No Production model found for '{model_name}'.\")\n",
    "        return None\n",
    "    \n",
    "    production = prod_versions[0]\n",
    "\n",
    "    model = load_model_by_version(model_name, production.version)\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"model_name\": model_name,\n",
    "        \"model_version\": production.version,\n",
    "        \"run_id\": production.run_id\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2067c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    return precision_score(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80114a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91123c629261458b8bbf243d62f92720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/17 12:18:01 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - cloudpickle (current: 3.1.2, required: cloudpickle==3.1.1)\n",
      " - numpy (current: 2.3.5, required: numpy==2.4.1)\n",
      " - pyarrow (current: 22.0.0, required: pyarrow==18.1.0)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': mlflow.pyfunc.loaded_model:\n",
      "  artifact_path: mlflow-artifacts:/0/models/m-bb67f34ce1a74004b765e7167f26905e/artifacts\n",
      "  flavor: mlflow.sklearn\n",
      "  run_id: 16b4d325e0924c03a145dd1f413f106a\n",
      ",\n",
      " 'model_name': 'eur_usd_direction_model',\n",
      " 'model_version': '1',\n",
      " 'run_id': '16b4d325e0924c03a145dd1f413f106a'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "challenger_model_data = load_challenger_model(model_name=\"eur_usd_direction_model\")\n",
    "pprint(challenger_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16da3159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No Production model found for 'eur_usd_direction_model'.\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/17wd4myj387b056cpgkkpxth0000gn/T/ipykernel_62110/572947344.py:2: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  prod_versions = client.get_latest_versions(name=model_name, stages=[\"Production\"])\n"
     ]
    }
   ],
   "source": [
    "production_model_data = load_production_model(model_name=\"eur_usd_direction_model\")\n",
    "pprint(production_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4aa51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promote(model_name, challenger, production):\n",
    "    if production:\n",
    "        client.transition_model_version_stage(\n",
    "            name=production[\"model_name\"],\n",
    "            version=production[\"model_version\"],\n",
    "            stage=\"Archived\"\n",
    "        )\n",
    "\n",
    "    client.transition_model_version_stage(\n",
    "        name=challenger[\"model_name\"],\n",
    "        version=challenger[\"model_version\"],\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "\n",
    "    client.set_model_version_tag(\n",
    "        name=challenger[\"model_name\"],\n",
    "        version=challenger[\"model_version\"],\n",
    "        key=\"candidate\",\n",
    "        value=\"champion\"\n",
    "    )\n",
    "    print(f\"Promoted version {challenger[\"model_version\"]} to Production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "241eed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def archive_challenger(model_name, challenger):\n",
    "    client.transition_model_version_stage(\n",
    "        name=challenger[\"model_name\"],\n",
    "        version=challenger[\"model_version\"],\n",
    "        stage=\"Archived\"\n",
    "    )\n",
    "    print(f\"Archived challenger version {challenger[\"model_version\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9980232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.data import from_pandas\n",
    "\n",
    "def promote_if_better(model_name, X_test, y_test):\n",
    "    challenger = load_challenger_model(model_name)\n",
    "    production = load_production_model(model_name)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"model_promotion_evaluation\"):\n",
    "        challenger_score = evaluate(challenger[\"model\"], X_test, y_test)\n",
    "        mlflow.log_metric(\"challenger_test_precision_score\", challenger_score)\n",
    "\n",
    "        test_df = X_test.copy()\n",
    "        test_df[\"target\"] = y_test \n",
    "\n",
    "        test_dataset = from_pandas(\n",
    "            test_df,\n",
    "            source=\"eur_usd_test\"\n",
    "        )\n",
    "        mlflow.log_input(\n",
    "            test_dataset,\n",
    "            context=\"testing\"\n",
    "        )\n",
    "\n",
    "        if production:\n",
    "            production_score = evaluate(production[\"model\"], X_test, y_test)\n",
    "        else:\n",
    "            production_score = None \n",
    "        mlflow.log_metric(\"production_test_precision_score\", production_score)\n",
    "\n",
    "        mlflow.log_param(\"challenger_version\", challenger[\"model_version\"])\n",
    "        if production:\n",
    "            mlflow.log_param(\"production_version\", production[\"model_version\"])\n",
    "\n",
    "        print(f\"Challenger score: {challenger_score}\")\n",
    "        print(f\"Production score: {production_score}\")\n",
    "\n",
    "        decision = (\n",
    "            \"promote_challenger\" \n",
    "            if production_score is None or challenger_score > production_score\n",
    "            else \"retain_production\"\n",
    "        )\n",
    "        mlflow.log_param(\"promotion_decision\", decision)\n",
    "\n",
    "    if decision == \"promote_challenger\":\n",
    "        promote(challenger, production)\n",
    "        return f\"challenger promoted '{challenger[\"model_name\"]}'\"\n",
    "    else:\n",
    "        archive_challenger(challenger)\n",
    "        return \"production model retained\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66b74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forex-forecasting (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
